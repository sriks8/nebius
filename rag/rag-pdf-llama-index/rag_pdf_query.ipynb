{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOuPGNLs0Rzh"
      },
      "source": [
        "# Query PDF documents using RAG (Llama-Index + Nebius AI)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nebius/token-factory-cookbook/blob/main/rag/rag-pdf-llama-index/rag_pdf_query.ipynb)\n",
        "[![](https://img.shields.io/badge/Powered%20by-Nebius%20AI-orange?style=flat&labelColor=orange&color=green)](http://tokenfactory.nebius.com/)\n",
        "\n",
        "This example shows querying a PDF using  [llama index](https://docs.llamaindex.ai/en/stable/) framework and running LLM on [Nebius Token Factory](https://tokenfactory.nebius.com/)\n",
        "\n",
        "[Read more about it here](https://github.com/nebius/token-factory-cookbook/blob/main/rag/rag-pdf-llama-index/README.md)\n",
        "\n",
        "\n",
        "## References and Acknowledgements\n",
        "\n",
        "- [llamaindex documentation](https://docs.llamaindex.ai/en/stable/)\n",
        "- [Nebius Token Factory](https://tokenfactory.nebius.com/)\n",
        "- [Nebius Token Factory documentation](https://docs.tokenfactory.nebius.com//inference/quickstart)\n",
        "\n",
        "## Pre requisites\n",
        "\n",
        "- Nebius API key.  Sign up for free at [Token Factory](https://tokenfactory.nebius.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idu8ref1XEri"
      },
      "source": [
        "## 2 - Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBn7wla7YeVl",
        "outputId": "3925d899-3fa8-411f-d931-00e808568a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "   RUNNING_ON_COLAB = True\n",
        "   print(\"Running in Colab\")\n",
        "else:\n",
        "  RUNNING_ON_COLAB = False\n",
        "  print(\"NOT Running in Colab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "WdH7Q0uNXJ3u"
      },
      "outputs": [],
      "source": [
        "if RUNNING_ON_COLAB:\n",
        "  # Install the required packages\n",
        "  !pip install -q llama-index-llms-litellm \\\n",
        "                  llama-index-llms-nebius \\\n",
        "                  llama-index-embeddings-nebius \\\n",
        "                  llama-index-embeddings-huggingface \\\n",
        "                  python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHr1sLWuX7e3"
      },
      "source": [
        "## 3 - Load Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk4FU9keX8L6",
        "outputId": "1b4d0aae-6ba4-4e88-d9ab-e7b6fdae4f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ NEBIUS_API_KEY found\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "\n",
        "## Recommended way of getting configuration\n",
        "if RUNNING_ON_COLAB:\n",
        "   from google.colab import userdata\n",
        "   NEBIUS_API_KEY = userdata.get('NEBIUS_API_KEY')\n",
        "else:\n",
        "   from dotenv import load_dotenv\n",
        "   load_dotenv()\n",
        "   NEBIUS_API_KEY = os.getenv('NEBIUS_API_KEY')\n",
        "\n",
        "\n",
        "## quick hack (not recommended) - you can hardcode the config key here\n",
        "# NEBIUS_API_KEY = \"your_key_here\"\n",
        "\n",
        "if NEBIUS_API_KEY:\n",
        "  print ('✅ NEBIUS_API_KEY found')\n",
        "  os.environ['NEBIUS_API_KEY'] = NEBIUS_API_KEY\n",
        "else:\n",
        "  raise RuntimeError ('❌ NEBIUS_API_KEY NOT found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nI0FzILYVQ-"
      },
      "source": [
        "## 4 - Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdc6njBCYXo_"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "input_dir = 'data'\n",
        "\n",
        "if RUNNING_ON_COLAB:\n",
        "    shutil.os.makedirs(input_dir, exist_ok=True)\n",
        "    !wget -O  '{input_dir}/attention.pdf' 'https://github.com/sriks8/nebius/blob/main/attn.pdf'\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pNQ2ozxgooz"
      },
      "source": [
        "## 5 - Setup Embedding Model\n",
        "\n",
        "We have a choice of local embedding model (fast) or running it on the cloud\n",
        "\n",
        "If running locally:\n",
        "- choose smaller models\n",
        "- less accuracy but faster\n",
        "\n",
        "If running on the cloud\n",
        "- We can run large models (billions of params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "UBOBBKorgvTb"
      },
      "outputs": [],
      "source": [
        "## Local model\n",
        "import os\n",
        "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "## Running embedding models on Nebius cloud\n",
        "# from llama_index.embeddings.nebius import NebiusEmbedding\n",
        "# Settings.embed_model = NebiusEmbedding(\n",
        "#                         model_name='BAAI/bge-en-icl',\n",
        "#                         api_key=os.getenv(\"NEBIUS_API_KEY\") # if not specfified here, it will get taken from env variable\n",
        "#                        )\n",
        "\n",
        "## Try out a few open source embedding models locally\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    # model_name = 'sentence-transformers/all-MiniLM-L6-v2' # 23 M params\n",
        "    model_name = 'BAAI/bge-small-en-v1.5'  # 33M params\n",
        "    # model_name = 'Qwen/Qwen3-Embedding-0.6B'  # 600M params\n",
        "    # model_name = 'BAAI/bge-en-icl'  # 7B params\n",
        "    #model_name = 'intfloat/multilingual-e5-large-instruct'  # 560M params\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lKdlYukZ2h_"
      },
      "source": [
        "## 6 - Setup LLama Index with Nebius\n",
        "\n",
        "We can use `llama_index.llms.nebius.NebiusLLM` or `llama_index.llms.litellm.LiteLLM`.\n",
        "\n",
        "See examples below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm5YUtNHaPNY"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.nebius import NebiusLLM\n",
        "from llama_index.llms.litellm import LiteLLM\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = NebiusLLM(\n",
        "                model='meta-llama/Llama-3.3-70B-Instruct',\n",
        "                # model='deepseek-ai/DeepSeek-R1-0528',\n",
        "                # model='openai/gpt-oss-20b',\n",
        "                api_key=os.getenv(\"NEBIUS_API_KEY\") # if not specfified, it will get taken from env variable\n",
        "    )\n",
        "\n",
        "# Settings.llm = LiteLLM(\n",
        "#                 model='nebius/meta-llama/Llama-3.3-70B-Instruct',\n",
        "#                 model='nebius/deepseek-ai/DeepSeek-R1-0528',\n",
        "#                 model='nebius/openai/gpt-oss-20b',\n",
        "#                 api_key=os.getenv(\"NEBIUS_API_KEY\") # if not specfified, it will get taken from env variable\n",
        "#     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIsbPCkCbh6G"
      },
      "source": [
        "## 6 - Read PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyVEm2Q4cGAM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "pattern = os.path.join(input_dir, '*.pdf')\n",
        "input_file_count = len(glob.glob(pattern, recursive=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ9FHpUvblJ9",
        "outputId": "9296a36c-e26e-45c3-986e-3d9aebe245f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 docs from 1 files\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(input_dir).load_data()\n",
        "print (f'Loaded {len(documents)} docs from {input_file_count} files')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiF4PBdrdH8R",
        "outputId": "4eef9e63-a8c1-4703-fb95-638d7cd5320c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 11.6 s, sys: 344 ms, total: 12 s\n",
            "Wall time: 9.78 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSic2nxNkpCn"
      },
      "source": [
        "##  7 - Query documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "27Xi4E-mksRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f013e4-2eb2-4f14-9172-82f3961007ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Hmm, the user is asking about the attention mechanism in machine learning. Looking at the provided context, I see detailed technical explanations from what appears to be a research paper. \n",
            "\n",
            "The context describes two key aspects: Scaled Dot-Product Attention and Multi-Head Attention. For Scaled Dot-Product Attention, it explains how it works with queries, keys and values - computing dot products, scaling by the square root of dimension, and applying softmax. The mathematical formulation is given as a matrix operation. There's also comparison to additive attention, explaining why scaling is necessary for larger dimensions.\n",
            "\n",
            "For Multi-Head Attention, the context mentions it involves multiple parallel attention layers with linear projections of queries, keys and values. This allows the model to jointly attend to information from different representation subspaces.\n",
            "\n",
            "The visualization example shows how attention heads can capture long-distance dependencies in text, like relating \"making\" to \"more difficult\" across a sentence. This demonstrates the mechanism's ability to handle contextual relationships.\n",
            "\n",
            "I should synthesize this information into a clear definition without directly referencing the context. The explanation should cover both the computational process and practical benefits like handling long-range dependencies. Since the user didn't specify their expertise level, I'll keep it technically accurate but avoid excessive jargon where possible.\n",
            "\n",
            "The answer should flow from basic concept to implementation details, ending with the real-world linguistic example to illustrate its value. I notice the context emphasizes efficiency advantages over additive attention too - that's worth including as it explains why this approach became dominant.\n",
            "</think>\n",
            "Attention mechanism is a computational approach that enables models to selectively focus on relevant parts of input data when processing information. It operates by assigning different weights to input elements based on their importance for a given task.\n",
            "\n",
            "The core process involves three components: queries (representing current focus), keys (identifying input features), and values (containing actual content). Compatibility between queries and keys determines attention weights, which then weight the values to produce context-aware outputs.\n",
            "\n",
            "A common implementation is scaled dot-product attention, which calculates compatibility scores through dot products between queries and keys. These scores are scaled by the inverse square root of the key dimension to maintain stable gradients, then normalized via softmax to create weighting distributions. The final output combines these weights with the value vectors.\n",
            "\n",
            "Multi-head attention extends this concept by employing multiple parallel attention layers. Each head applies independent linear transformations to the queries, keys, and values, allowing the model to capture different types of relationships simultaneously. The outputs from all heads are combined to form the final result.\n",
            "\n",
            "This mechanism excels at identifying long-range dependencies in data, such as connecting distant words in sentences (e.g., linking \"making\" to \"more difficult\" across phrases). By dynamically weighting relevant information, attention improves contextual understanding and pattern recognition in sequential data processing tasks.\n"
          ]
        }
      ],
      "source": [
        "response = index.as_query_engine().query(\"What is attention mechanism?\")\n",
        "print (response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmKZ73RcqHrk",
        "outputId": "e2f1bdfb-7068-49d3-d93e-45972ced7896"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'31b40151-7d77-440a-91e3-5911a070dff9': {'file_path': '/home/sujee/my-stuff/projects/nebius/token-factory-cookbook-1/rag/rag-pdf-llama-index/data/attention.pdf',\n",
              "  'file_name': 'attention.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 2215244,\n",
              "  'creation_date': '2025-07-07',\n",
              "  'last_modified_date': '2025-07-07'},\n",
              " '0409e07f-8930-4a01-806d-b94ce9301994': {'file_path': '/home/sujee/my-stuff/projects/nebius/token-factory-cookbook-1/rag/rag-pdf-llama-index/data/attention.pdf',\n",
              "  'file_name': 'attention.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 2215244,\n",
              "  'creation_date': '2025-07-07',\n",
              "  'last_modified_date': '2025-07-07'}}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# see where the answer came from\n",
        "response.metadata"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag-pdf-llama-index",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}